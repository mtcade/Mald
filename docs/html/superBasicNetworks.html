<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>&lt;no title&gt; &#8212; Mald 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=61cd365c" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=12dfc556" />
    <script src="_static/documentation_options.js?v=d45e8c67"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="&lt;no title&gt;" href="nnImportance.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p id="module-maldImportance.superBasicNetworks">Interface for creating simple Keras neural networks for outcome variables, and calculating the Mean Absolute Local Derivatives (MALD)</p>
<p>If you have your own preferred networks or other predictors, you can skip using this module.</p>
<dl class="py function">
<dt class="sig sig-object py" id="maldImportance.superBasicNetworks.fit_SimpleNN">
<span class="sig-prename descclassname"><span class="pre">maldImportance.superBasicNetworks.</span></span><span class="sig-name descname"><span class="pre">fit_SimpleNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xk</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_first</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_layer_width</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_shrink_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#maldImportance.superBasicNetworks.SimpleNN" title="maldImportance.superBasicNetworks.SimpleNN"><span class="pre">SimpleNN</span></a></span></span><a class="headerlink" href="#maldImportance.superBasicNetworks.fit_SimpleNN" title="Link to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>np.ndarray</em><em>|</em><em>pd.DataFrame</em>) – Explanatory data</p></li>
<li><p><strong>Xk</strong> (<em>np.ndarray</em><em>|</em><em>pd.DataFrame</em>) – Knockoff data, of the same dimension as <cite>X</cite></p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>|</em><em>pd.Series</em>) – Outcome data</p></li>
<li><p><strong>drop_first</strong> (<em>bool</em>) – How to handle one-hot-encoding categorical variables. If <cite>True</cite> the number of associated columns is the number of categories minus 1.</p></li>
<li><p><strong>save_root</strong> (<em>str</em>) – Dir to make saves</p></li>
<li><p><strong>save_name</strong> (<em>str</em>) – Name to use for saving checkpoints</p></li>
<li><p><strong>epochs</strong> (<em>int</em>) – Epochs for <cite>.fit()</cite></p></li>
<li><p><strong>dense_activation</strong> (<em>str</em>) – Activation of internal layers, most likely ‘relu,’ ‘sigmoid’, ‘leaky_relu’, etc</p></li>
<li><p><strong>first_layer_width</strong> (<em>float</em><em>|</em><em>int</em>) – Size of first layer after input. If an integer, it uses that value. If a float, it’s a multiplier of the input size.</p></li>
<li><p><strong>layers</strong> (<em>int</em>) – Number of internal layers</p></li>
<li><p><strong>layer_shrink_factor</strong> (<em>float</em>) – With multiple internal layers, the width of each is this multiplied by the previous layer’s width</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em>) – Learning rate for <cite>.fit()</cite>; common values are 0.01, 0.005, 0.001, 0.0005, 0.0001</p></li>
<li><p><strong>verbose</strong> (<em>int</em>) – How much to print out, for mostly for debugging.</p></li>
</ul>
</dd>
</dl>
<p>Initializing and fits a <code class="docutils literal notranslate"><span class="pre">SimpleNN</span></code> with defaults.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="maldImportance.superBasicNetworks.get_SimpleNN">
<span class="sig-prename descclassname"><span class="pre">maldImportance.superBasicNetworks.</span></span><span class="sig-name descname"><span class="pre">get_SimpleNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_layer_width</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_shrink_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#maldImportance.superBasicNetworks.SimpleNN" title="maldImportance.superBasicNetworks.SimpleNN"><span class="pre">SimpleNN</span></a></span></span><a class="headerlink" href="#maldImportance.superBasicNetworks.get_SimpleNN" title="Link to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_root</strong> (<em>str</em>) – Dir to make saves</p></li>
<li><p><strong>save_name</strong> (<em>str</em>) – Name to use for saving checkpoints</p></li>
<li><p><strong>epochs</strong> (<em>int</em>) – Epochs for <cite>.fit()</cite></p></li>
<li><p><strong>dense_activation</strong> (<em>str</em>) – Activation of internal layers, most likely ‘relu,’ ‘sigmoid’, ‘leaky_relu’, etc</p></li>
<li><p><strong>first_layer_width</strong> (<em>float</em><em>|</em><em>int</em>) – Size of first layer after input. If an integer, it uses that value. If a float, it’s a multiplier of the input size.</p></li>
<li><p><strong>layers</strong> (<em>int</em>) – Number of internal layers</p></li>
<li><p><strong>layer_shrink_factor</strong> (<em>float</em>) – With multiple internal layers, the width of each is this multiplied by the previous layer’s width</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em>) – Learning rate for <cite>.fit()</cite>; common values are 0.01, 0.005, 0.001, 0.0005, 0.0001</p></li>
<li><p><strong>verbose</strong> (<em>int</em>) – How much to print out, for mostly for debugging.</p></li>
</ul>
</dd>
</dl>
<p>Constructor for <code class="docutils literal notranslate"><span class="pre">SimpleNN</span></code>, setting defaults</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="maldImportance.superBasicNetworks.SimpleNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">maldImportance.superBasicNetworks.</span></span><span class="sig-name descname"><span class="pre">SimpleNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_layer_width</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_shrink_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#maldImportance.superBasicNetworks.SimpleNN" title="Link to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_root</strong> (<em>str</em>) – Dir to make saves</p></li>
<li><p><strong>save_name</strong> (<em>str</em>) – Name to use for saving checkpoints</p></li>
<li><p><strong>epochs</strong> (<em>int</em>) – Epochs for <cite>.fit()</cite></p></li>
<li><p><strong>dense_activation</strong> (<em>str</em>) – Activation of internal layers, most likely ‘relu,’ ‘sigmoid’, ‘leaky_relu’, etc</p></li>
<li><p><strong>first_layer_width</strong> (<em>float</em><em>|</em><em>int</em>) – Size of first layer after input. If an integer, it uses that value. If a float, it’s a multiplier of the input size.</p></li>
<li><p><strong>layers</strong> (<em>int</em>) – Number of internal layers</p></li>
<li><p><strong>layer_shrink_factor</strong> (<em>float</em>) – With multiple internal layers, the width of each is this multiplied by the previous layer’s width</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em>) – Learning rate for <cite>.fit()</cite>; common values are 0.01, 0.005, 0.001, 0.0005, 0.0001</p></li>
<li><p><strong>verbose</strong> (<em>int</em>) – How much to print out, for mostly for debugging.</p></li>
</ul>
</dd>
</dl>
<p>See <a class="reference external" href="https://keras.io/api/models/">https://keras.io/api/models/</a></p>
<p>Easy interface for dense, simply connected Neural Networks, a good starting point for estimation of single continuous outcomes of i.i.d. explanatory data. The key is that this class conforms to the <cite>PredictionModel</cite> Protocol, used by other modules, by including:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>.fit()</cite></p></li>
<li><p><cite>.predict()</cite></p></li>
<li><p><cite>.call()</cite></p></li>
</ul>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="maldImportance.superBasicNetworks.SimpleNN.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></span><a class="headerlink" href="#maldImportance.superBasicNetworks.SimpleNN.call" title="Link to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> (<em>np.ndarray</em><em>|</em><em>tf.Tensor</em>) – Explanatory data, likely including the knockoffs</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Prediction result from <cite>self.network.call(X)</cite></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>np.ndarray|tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="maldImportance.superBasicNetworks.SimpleNN.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#maldImportance.superBasicNetworks.SimpleNN.fit" title="Link to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>np.ndarray</em>) – Explanatory data, likely including the knockoffs</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em>) – Outcome data</p></li>
</ul>
</dd>
</dl>
<p>Initializes the <cite>.network</cite> if necessary and calls <cite>.fit</cite> on it</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="maldImportance.superBasicNetworks.SimpleNN.getNetwork">
<span class="sig-name descname"><span class="pre">getNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hyperparameters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Model</span></span></span><a class="headerlink" href="#maldImportance.superBasicNetworks.SimpleNN.getNetwork" title="Link to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_length</strong> (<em>int</em>) – Dimension of explanatory input data</p></li>
<li><p><strong>hyperparamters</strong> (<em>dict</em><em>[ </em><em>str</em><em>, </em><em>any</em><em> ]</em>) – Network architecture and training hyperparamters.</p></li>
</ul>
</dd>
</dl>
<p>Builds a network with architecture from hyperparameters (see <cite>get_simpleNN()</cite>)</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>layers int</cite>: Number of dense layers</p></li>
<li><p><cite>first_layer_width int|float</cite>: if int, layer width. If float, a multiplier of input_length</p></li>
<li><p><cite>layer_shrink_factor float</cite>: Multiplier for layer width</p></li>
<li><p><cite>learning_rate float</cite></p></li>
</ul>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="maldImportance.superBasicNetworks.SimpleNN.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="headerlink" href="#maldImportance.superBasicNetworks.SimpleNN.predict" title="Link to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> (<em>np.ndarray</em>) – Explanatory data, likely including the knockoffs</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Predictions from the trained network</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
<p>Calls <cite>self.network.predict( X )</cite></p>
</dd></dl>

</dd></dl>



          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Mald</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="importance.html"><code class="docutils literal notranslate"><span class="pre">auto_diff()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="importance.html#maldImportance.importance.importancesFromModel"><code class="docutils literal notranslate"><span class="pre">importancesFromModel()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="importance.html#maldImportance.importance.wFromImportances"><code class="docutils literal notranslate"><span class="pre">wFromImportances()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="importance.html#maldImportance.importance.wFromModel"><code class="docutils literal notranslate"><span class="pre">wFromModel()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="importance.html#maldImportance.importance.PredictionModel"><code class="docutils literal notranslate"><span class="pre">PredictionModel</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="nnImportance.html"><code class="docutils literal notranslate"><span class="pre">importances()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="nnImportance.html#maldImportance.nnImportance.wStats"><code class="docutils literal notranslate"><span class="pre">wStats()</span></code></a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#"><code class="docutils literal notranslate"><span class="pre">fit_SimpleNN()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="#maldImportance.superBasicNetworks.get_SimpleNN"><code class="docutils literal notranslate"><span class="pre">get_SimpleNN()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="#maldImportance.superBasicNetworks.SimpleNN"><code class="docutils literal notranslate"><span class="pre">SimpleNN</span></code></a><ul>
<li class="toctree-l2"><a class="reference internal" href="#maldImportance.superBasicNetworks.SimpleNN.call"><code class="docutils literal notranslate"><span class="pre">SimpleNN.call()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#maldImportance.superBasicNetworks.SimpleNN.fit"><code class="docutils literal notranslate"><span class="pre">SimpleNN.fit()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#maldImportance.superBasicNetworks.SimpleNN.getNetwork"><code class="docutils literal notranslate"><span class="pre">SimpleNN.getNetwork()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#maldImportance.superBasicNetworks.SimpleNN.predict"><code class="docutils literal notranslate"><span class="pre">SimpleNN.predict()</span></code></a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="nnImportance.html" title="previous chapter">&lt;no title&gt;</a></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Evan Mason.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.3.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="_sources/superBasicNetworks.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>